{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9c5144-66b1-4ac4-8cb0-63699ab73df2",
   "metadata": {},
   "source": [
    "# Sentence Transformers and Multi-Task Learning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8ff2e99-14a4-4b0a-9bb5-9892ae3aa029",
   "metadata": {},
   "source": [
    "The following notebook implements some basic examples of using a transformer for computing sentence similarity as well as a couple of additional NLP tasks.\n",
    "\n",
    "I put the code primarily in a Jupyter Notebook, with the exception of the requirements.txt. Normally for production code, you'd put the classes in their own modules and only use notebooks to share experiments, etc. But for this exercise I think putting everything in here (with code, comments, and output all in sequence) provides the clearest way to explain my thought process.\n",
    "\n",
    "I used Huggingface Transformers to get the backbone model. I opted not to use one of the popular Sentence Transformer wrappers such as SentenceTransformers because 1. I wasn't sure if it was cheating, haha, and 2. I thought it would be a little more interesting to show a couple more design elements. I'll describe those in comments down in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881a4a6-e182-4fd0-8b79-1b68d1d9c593",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d90275-e4f4-4e20-851a-e9781048b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53224121-e199-455f-8462-f47c7efba2f0",
   "metadata": {},
   "source": [
    "## Sentence Transformer for Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843a6046-5328-4d36-8014-a28024ec27cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This could pretty much any language model from HuggingFace, but this is nice and small for demo purposes\n",
    "# Realistically, sentence embedding similarity without task-specific training is a tough job and you'd probably\n",
    "# want a decent-sized model.\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e2a76d-d366-4763-8a28-c53cfb51de1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e367ce-6435-4f5c-9563-b37a018e2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Simple wrapper to run a model and tokenizer on one or more text inputs to calculate and compare embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given a text input, produce its feature embedding (vector). This implementation uses the last 4 hidden layers of the model.\n",
    "        After performing average pooling on each of the tokens of that hidden state, the results are concatenated into a single vector.\n",
    "        \"\"\"\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        outputs = model(**encoded_input, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        USE_LAST_N_LAYERS = 4  # Some research suggests that there is complementary embedding information in different hidden layers, so we can use a few\n",
    "        pooled_layer_embeddings = []\n",
    "        for i in range(1, USE_LAST_N_LAYERS+1):\n",
    "            hidden_state = hidden_states[-i][0].detach().numpy()\n",
    "            # There are many ways to go from n output tokens down to a fixed-length embedding\n",
    "            # For certain pre-trained models there is even a recommended output token to use exclusively\n",
    "            # But average pooling over all output tokens should be a fairly safe model-agnostic approach\n",
    "            pooled_hidden_state = np.mean(hidden_state, axis=0) \n",
    "            pooled_layer_embeddings.append(pooled_hidden_state)\n",
    "        \n",
    "        return np.concatenate(pooled_layer_embeddings)\n",
    "\n",
    "    def get_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        Given two text inputs, produce their similarity score (based on cosine similarity)\n",
    "        \"\"\"\n",
    "        embedding1 = self.get_embedding(text1)\n",
    "        embedding2 = self.get_embedding(text2)\n",
    "        return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd61b35-81f2-4bd3-8c53-1ee3e04eec27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple usage example\n",
    "sentence_transformer = SentenceTransformer(model=model, tokenizer=tokenizer)\n",
    "embedding = sentence_transformer.get_embedding(\"Well, hello there\")\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198edde4-e5ff-463b-ac87-d01c347a56df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87508154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These sentences should hopefully have a fairly high similarity\n",
    "sentence_transformer.get_similarity(\"Well, hello there\", \"Hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce5d23f-a9ab-4e7a-b529-56f1057434e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81927824"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These sentences should hopefully have lower similarity than the last.\n",
    "# But, we aren't using a very powerful model so the difference is smaller than ideal\n",
    "sentence_transformer.get_similarity(\"Well, hello there\", \"I love pancakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98131724-ea4c-4a06-bc56-b2ccc0d127ff",
   "metadata": {},
   "source": [
    "## Multi-Task Learning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81d2aabe-5581-486e-af97-82957b29c801",
   "metadata": {},
   "source": [
    "This step shows how to implement a multi-task model that includes both a sequence classification head as well a named entity recognition head. It seemed good to show both a per-sequence output (sequence classification) and a per-token output (NER).\n",
    "\n",
    "The architecture simply consists of a backbone model, and task-specific linear layers. In the sequence classification case, we use the first token-level output from the backbone so that we have a fixed-length output, and then the linear layer can be applied. Pooling is generally not used for adding a sequence classification head (but, it might be worth the trouble if you freeze the backbone model).\n",
    "\n",
    "I omitted a task-specific hidden layer, but if a task was performing poorly I would potentially consider adding such a linear layer, followed by a non-linearity and then the final task-specific linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9dec1bb-6362-4392-9ce3-36abf9f35e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple wrapper to run a model and tokenizer on one or more text inputs to calculate and compare embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone: nn.Module, num_sequence_classes: int, num_ner_classes: int):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        # Note: this strategy to get the final output size will not work for all backbone modules,\n",
    "        # as unfortunately it is not standard across all Hugging Face models\n",
    "        # It will work for Distilbert though\n",
    "        final_hidden_size = backbone.config.dim\n",
    "        self.classification_head = nn.Linear(final_hidden_size, num_sequence_classes)\n",
    "        num_ner_labels = 2 * num_ner_classes + 1 # With IOB labeling, we need \"I\"(nside) and \"B\"(egin) labels for each class, plus an \"O\"(utside) label\n",
    "        self.ner_head = nn.Linear(final_hidden_size, num_ner_labels)\n",
    "\n",
    "    def forward(self, \n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # I'm leaving out a lot of possible configurability that you would get in, like, a Hugging Face implementation\n",
    "        # But input token ids and attention mask should be enough to do basic forward/backward passes with batching for training\n",
    "        # I've also left out calculation of loss. You could put that inside or outside the model,\n",
    "        # but either way we don't need it for forward() only\n",
    "\n",
    "        # Get the last hidden state as input to the task layers\n",
    "        backbone_output = self.backbone(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        # Get the first token output to use for sequence classification\n",
    "        first_token_output = backbone_output[:, 0]\n",
    "        sequence_classification_logits = self.classification_head(first_token_output)\n",
    "\n",
    "        # Get ner token predictions, per token\n",
    "        # Note that to actually label a sentence with this model you'd need to convert IOB labels to annotations but this isn't hard\n",
    "        ner_logits = self.ner_head(backbone_output)\n",
    "        \n",
    "        return sequence_classification_logits, ner_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f11d2fbc-de43-4cc9-9b64-c278ab6686c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 4, 11]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_task_transformer = MultiTaskTransformer(backbone=model, num_sequence_classes=3, num_ner_classes=5)\n",
    "encoded_input = tokenizer(\"hello there\", return_tensors='pt')\n",
    "sequence_classification_logits, ner_logits = multi_task_transformer(**encoded_input)\n",
    "# This should be [1, num_sequence_classes], [1, token_count, num_ner_labels]\n",
    "sequence_classification_logits.shape, ner_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a446675-3fa1-40f9-9c67-09d1346e5bd0",
   "metadata": {},
   "source": [
    "## Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdf5a8-f5fe-4b53-9c3c-67c5ba35f979",
   "metadata": {},
   "source": [
    "*1. Consider the scenario of training the multi-task sentence transformer that you\n",
    "implemented in Task 2. Specifically, discuss how you would decide which portions of the\n",
    "network to train and which parts to keep frozen. For example,*\n",
    "- *When would it make sense to freeze the transformer backbone and only train the\n",
    "task specific layers?*\n",
    "- *When would it make sense to freeze one head while training the other?*\n",
    "\n",
    "My default plan, if I only got to try one thing, would be to leave all layers unfrozen. The main advantage of freezing any layers is to prevent overfitting and/or catastrophic forgetting. In my personal experience (especially with this newer, larger LLMs that are highly overparameterized and are fine-tuned with very low learning rates) you can prevent those problems by training for a small number of epochs, and perhaps adding some regularization like dropout if you really happen to need it. Freezing the layers can occasionally make the training pretty constrained and unable to fit the data very easily, depending on how much of it you have. Another possibility short of a full freeze is to do LoRA training, which will speed up big model training and constrain the magnitude of a model update without restricting it as much as fully freezing it.\n",
    "\n",
    "If you had precious little data to train on, I could maybe see freezing the backbone as a good option, and I'm certainly not opposed to trying it.\n",
    "\n",
    "You could freeze one head while training the other if you wanted. It would definitely make sense if you have disjoint training data in which case you can't update both objectives simultaneously even if you wanted to. Then you'd have to figure out a good schedule for training that interleaves samples of one task with samples of another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661dd11-3211-4daa-9d89-6f018aed7948",
   "metadata": {},
   "source": [
    "*2. Discuss how you would decide when to implement a multi-task model like the one in this\n",
    "assignment and when it would make more sense to use two completely separate models\n",
    "for each task.*\n",
    "\n",
    "Historically, I would have said model capacity is the main determining factor whether to use two models. That is, two highly disparate tasks might not be easily contained in one model (but two related tasks might benefit from the shared learnings!). Now, the models are so big and the baseline language understanding that you need for most tasks is so similar, that deciding whether to use two models or one tends to be dictated, in my opinion, by more classical engineering considerations.\n",
    "\n",
    "It's easier to maintain two models (can update one task without having to regression test the other, which is a painful process in ML). But, two models is more expensive to run because you are deploying them separately (You'll need about double the GPUs, since it is about double the memory and double the compute. Latency-wise, it will be about a wash though)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483343e-fc50-4ade-824d-9763ee70c14e",
   "metadata": {},
   "source": [
    "*3. When training the multi-task model, assume that Task A has abundant data, while Task\n",
    "B has limited data. Explain how you would handle this imbalance.*\n",
    "\n",
    "Depending on how much the tasks have in common, Task B may benefit from the shared knowledge implicit in Task A training, which is an argument to try training them as a single multi-task model. Past that you will need to separately optimize the amount of training for each task, as it probably won't make sense to train with the same schedule (same epochs and learning rate) if the data doesn't line up. As a starting point, I'd probably optimize number of training epochs for each separately and then interleave the training so that the distributions are balanced across the whole training time. But there's other interesting things to try. Maybe you train task A first and see if that fine-tuning benefits Task B being trained subsequently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
